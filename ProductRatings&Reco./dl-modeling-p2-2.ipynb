{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-09T19:23:08.439488Z",
     "iopub.status.busy": "2024-06-09T19:23:08.439127Z",
     "iopub.status.idle": "2024-06-09T19:23:17.088192Z",
     "shell.execute_reply": "2024-06-09T19:23:17.087139Z",
     "shell.execute_reply.started": "2024-06-09T19:23:08.439459Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "merged_df = pd.read_csv('/kaggle/input/finaldf-csv/merged_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T19:23:17.090117Z",
     "iopub.status.busy": "2024-06-09T19:23:17.089818Z",
     "iopub.status.idle": "2024-06-09T19:23:17.339367Z",
     "shell.execute_reply": "2024-06-09T19:23:17.338304Z",
     "shell.execute_reply.started": "2024-06-09T19:23:17.090069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 484007 entries, 0 to 484006\n",
      "Data columns (total 14 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   rating             484007 non-null  float64\n",
      " 1   title_x            483887 non-null  object \n",
      " 2   text               483874 non-null  object \n",
      " 3   helpful_vote       484007 non-null  int64  \n",
      " 4   verified_purchase  484007 non-null  bool   \n",
      " 5   title_y            483998 non-null  object \n",
      " 6   average_rating     484007 non-null  float64\n",
      " 7   rating_number      484007 non-null  int64  \n",
      " 8   price              484007 non-null  float64\n",
      " 9   details            484007 non-null  object \n",
      " 10  x_length           484007 non-null  int64  \n",
      " 11  y_length           484007 non-null  int64  \n",
      " 12  de_length          484007 non-null  int64  \n",
      " 13  review_length      484007 non-null  int64  \n",
      "dtypes: bool(1), float64(3), int64(6), object(4)\n",
      "memory usage: 48.5+ MB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T19:24:24.466408Z",
     "iopub.status.busy": "2024-06-09T19:24:24.466054Z",
     "iopub.status.idle": "2024-06-09T19:24:25.313353Z",
     "shell.execute_reply": "2024-06-09T19:24:25.312273Z",
     "shell.execute_reply.started": "2024-06-09T19:24:24.466380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 484007 entries, 0 to 484006\n",
      "Data columns (total 14 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   rating             484007 non-null  float64\n",
      " 1   title_x            484007 non-null  object \n",
      " 2   text               484007 non-null  object \n",
      " 3   helpful_vote       484007 non-null  int64  \n",
      " 4   verified_purchase  484007 non-null  bool   \n",
      " 5   title_y            484007 non-null  object \n",
      " 6   average_rating     484007 non-null  float64\n",
      " 7   rating_number      484007 non-null  int64  \n",
      " 8   price              484007 non-null  float64\n",
      " 9   details            484007 non-null  object \n",
      " 10  x_length           484007 non-null  int64  \n",
      " 11  y_length           484007 non-null  int64  \n",
      " 12  de_length          484007 non-null  int64  \n",
      " 13  review_length      484007 non-null  int64  \n",
      "dtypes: bool(1), float64(3), int64(6), object(4)\n",
      "memory usage: 48.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Fill NaN values in text columns with an empty string\n",
    "text_columns = ['title_x', 'text', 'title_y', 'details']\n",
    "merged_df[text_columns] = merged_df[text_columns].fillna('')\n",
    "\n",
    "# Check for remaining missing values and handle them\n",
    "merged_df.fillna(0, inplace=True)\n",
    "\n",
    "# Now all columns should have the same number of non-null values\n",
    "print(merged_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-07T20:03:50.278409Z",
     "iopub.status.busy": "2024-06-07T20:03:50.278084Z",
     "iopub.status.idle": "2024-06-07T20:19:36.571837Z",
     "shell.execute_reply": "2024-06-07T20:19:36.570909Z",
     "shell.execute_reply.started": "2024-06-07T20:03:50.278385Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.180754178287367\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy.sparse\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Fill NaN values in text columns with an empty string\n",
    "text_columns = ['text', 'title_x', 'title_y', 'details']\n",
    "merged_df[text_columns] = merged_df[text_columns].fillna('')\n",
    "\n",
    "# Combine text columns into one for TF-IDF vectorization\n",
    "merged_df['combined_text'] = merged_df['text'] + ' ' + merged_df['title_x'] + ' ' + merged_df['title_y'] + ' ' + merged_df['details']\n",
    "\n",
    "# Use a smaller sample of the data for experimentation\n",
    "sample_df = merged_df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=300)  # Reduce max features to save memory\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(sample_df['combined_text'])\n",
    "\n",
    "# Normalize numerical features\n",
    "numerical_features = sample_df[['price', 'average_rating', 'rating_number', 'helpful_vote', 'x_length', 'y_length', 'de_length']]\n",
    "scaler = StandardScaler()\n",
    "normalized_numerical_features = scaler.fit_transform(numerical_features)\n",
    "\n",
    "# Convert normalized numerical features to sparse matrix\n",
    "normalized_numerical_features_sparse = scipy.sparse.csr_matrix(normalized_numerical_features)\n",
    "\n",
    "# Combine TF-IDF features with normalized numerical features (both sparse matrices)\n",
    "X_sparse = scipy.sparse.hstack([tfidf_matrix, normalized_numerical_features_sparse])\n",
    "\n",
    "# Define the target variable\n",
    "y = sample_df['rating'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sparse, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply RandomOverSampler to handle class imbalance\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train a Random Forest Regressor\n",
    "model = RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1)  # Use all cores\n",
    "model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print the Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Using RNNs for sequential Processing of Text Data and Observing difference over treating text as static "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T19:08:37.444412Z",
     "iopub.status.busy": "2024-06-08T19:08:37.444066Z",
     "iopub.status.idle": "2024-06-08T19:08:54.479702Z",
     "shell.execute_reply": "2024-06-08T19:08:54.478845Z",
     "shell.execute_reply.started": "2024-06-08T19:08:37.444384Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 19:08:39.794785: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-08 19:08:39.794885: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-08 19:08:39.919852: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate\n",
    "\n",
    "# Sample data\n",
    "sample_df = merged_df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Fill NaN values in text columns with an empty string\n",
    "text_columns = ['text', 'title_x']\n",
    "sample_df[text_columns] = sample_df[text_columns].fillna('')\n",
    "\n",
    "# Prepare text data\n",
    "tokenizer = Tokenizer(num_words=10000)  # Adjust vocabulary size as needed\n",
    "tokenizer.fit_on_texts(sample_df['text'] + ' ' + sample_df['title_x'])\n",
    "\n",
    "# Convert text to sequences\n",
    "text_sequences = tokenizer.texts_to_sequences(sample_df['text'])\n",
    "title_sequences = tokenizer.texts_to_sequences(sample_df['title_x'])\n",
    "\n",
    "# Pad sequences\n",
    "max_seq_length = 100  # Adjust sequence length as needed\n",
    "text_padded = pad_sequences(text_sequences, maxlen=max_seq_length, padding='post')\n",
    "title_padded = pad_sequences(title_sequences, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "# Prepare numerical features\n",
    "numerical_features = sample_df[['price', 'average_rating', 'rating_number', 'helpful_vote', 'x_length', 'y_length', 'de_length']]\n",
    "scaler = StandardScaler()\n",
    "normalized_numerical_features = scaler.fit_transform(numerical_features)\n",
    "\n",
    "# Define the target variable\n",
    "y = sample_df['rating'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_text_train, X_text_test, X_title_train, X_title_test, X_num_train, X_num_test, y_train, y_test = train_test_split(\n",
    "    text_padded, title_padded, normalized_numerical_features, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T19:10:44.861028Z",
     "iopub.status.busy": "2024-06-08T19:10:44.860289Z",
     "iopub.status.idle": "2024-06-08T19:12:55.413465Z",
     "shell.execute_reply": "2024-06-08T19:12:55.412493Z",
     "shell.execute_reply.started": "2024-06-08T19:10:44.860993Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ text_input          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ title_input         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">500,000</span> │ text_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">500,000</span> │ title_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">29,440</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">29,440</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ numerical_input     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">135</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│                     │                   │            │ numerical_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,704</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ text_input          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ title_input         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m50\u001b[0m)   │    \u001b[38;5;34m500,000\u001b[0m │ text_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m50\u001b[0m)   │    \u001b[38;5;34m500,000\u001b[0m │ title_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m29,440\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m29,440\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ numerical_input     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m135\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│                     │                   │            │ numerical_input[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,704\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,067,649</span> (4.07 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,067,649\u001b[0m (4.07 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,067,649</span> (4.07 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,067,649\u001b[0m (4.07 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1210/1210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - loss: 2.6064 - val_loss: 1.9668\n",
      "Epoch 2/10\n",
      "\u001b[1m1210/1210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - loss: 1.8712 - val_loss: 1.5120\n",
      "Epoch 3/10\n",
      "\u001b[1m1210/1210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - loss: 1.0483 - val_loss: 0.7466\n",
      "Epoch 4/10\n",
      "\u001b[1m1210/1210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - loss: 0.5918 - val_loss: 0.7007\n",
      "Epoch 5/10\n",
      "\u001b[1m1210/1210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - loss: 0.4554 - val_loss: 0.7389\n",
      "Epoch 6/10\n",
      "\u001b[1m1210/1210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - loss: 0.3778 - val_loss: 0.7769\n",
      "Epoch 7/10\n",
      "\u001b[1m1210/1210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - loss: 0.3327 - val_loss: 0.8024\n",
      "Epoch 8/10\n",
      "\u001b[1m1210/1210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - loss: 0.2718 - val_loss: 0.8189\n",
      "Epoch 9/10\n",
      "\u001b[1m1210/1210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - loss: 0.2375 - val_loss: 0.8240\n",
      "Epoch 10/10\n",
      "\u001b[1m1210/1210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - loss: 0.2111 - val_loss: 0.8589\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step\n",
      "Mean Squared Error: 0.8589455007460488\n"
     ]
    }
   ],
   "source": [
    "# Define the text input\n",
    "text_input = Input(shape=(max_seq_length,), name='text_input')\n",
    "title_input = Input(shape=(max_seq_length,), name='title_input')\n",
    "\n",
    "# Embedding layers\n",
    "embedding_dim = 50  # Adjust embedding dimensions as needed\n",
    "text_embedding = Embedding(input_dim=10000, output_dim=embedding_dim, input_length=max_seq_length)(text_input)\n",
    "title_embedding = Embedding(input_dim=10000, output_dim=embedding_dim, input_length=max_seq_length)(title_input)\n",
    "\n",
    "# LSTM layers\n",
    "text_lstm = LSTM(64)(text_embedding)\n",
    "title_lstm = LSTM(64)(title_embedding)\n",
    "\n",
    "# Numerical input\n",
    "numerical_input = Input(shape=(normalized_numerical_features.shape[1],), name='numerical_input')\n",
    "\n",
    "# Concatenate all features\n",
    "concatenated = Concatenate()([text_lstm, title_lstm, numerical_input])\n",
    "\n",
    "# Dense layers\n",
    "dense = Dense(64, activation='relu')(concatenated)\n",
    "output = Dense(1)(dense)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[text_input, title_input, numerical_input], outputs=output)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    [X_text_train, X_title_train, X_num_train], y_train,\n",
    "    validation_data=([X_text_test, X_title_test, X_num_test], y_test),\n",
    "    epochs=10, batch_size=32)  # Adjust epochs and batch size as needed\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict([X_text_test, X_title_test, X_num_test])\n",
    "\n",
    "# Calculate and print the Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T19:25:01.595684Z",
     "iopub.status.busy": "2024-06-08T19:25:01.594845Z",
     "iopub.status.idle": "2024-06-08T19:25:01.601652Z",
     "shell.execute_reply": "2024-06-08T19:25:01.600487Z",
     "shell.execute_reply.started": "2024-06-08T19:25:01.595650Z"
    }
   },
   "source": [
    "<h2> Hyperparameter Tuned Feed Forward Networks </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T19:31:30.036262Z",
     "iopub.status.busy": "2024-06-08T19:31:30.035870Z",
     "iopub.status.idle": "2024-06-08T19:45:32.020294Z",
     "shell.execute_reply": "2024-06-08T19:45:32.019317Z",
     "shell.execute_reply.started": "2024-06-08T19:31:30.036238Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 01m 06s]\n",
      "mse: 0.3182298094034195\n",
      "\n",
      "Best mse So Far: 0.0954609289765358\n",
      "Total elapsed time: 00h 12m 54s\n",
      "\n",
      "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
      "layer is 32 and the optimal dropout rate is 0.0.\n",
      "\n",
      "Epoch 1/20\n",
      "\u001b[1m968/968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 2.9942 - mse: 2.9942 - val_loss: 0.8039 - val_mse: 0.8039\n",
      "Epoch 2/20\n",
      "\u001b[1m968/968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.5956 - mse: 0.5956 - val_loss: 0.7447 - val_mse: 0.7447\n",
      "Epoch 3/20\n",
      "\u001b[1m968/968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.3845 - mse: 0.3845 - val_loss: 0.7734 - val_mse: 0.7734\n",
      "Epoch 4/20\n",
      "\u001b[1m968/968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.2866 - mse: 0.2866 - val_loss: 0.7612 - val_mse: 0.7612\n",
      "Epoch 5/20\n",
      "\u001b[1m968/968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.2085 - mse: 0.2085 - val_loss: 0.8038 - val_mse: 0.8038\n",
      "Epoch 6/20\n",
      "\u001b[1m968/968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1771 - mse: 0.1771 - val_loss: 0.8151 - val_mse: 0.8151\n",
      "Epoch 7/20\n",
      "\u001b[1m968/968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1342 - mse: 0.1342 - val_loss: 0.8365 - val_mse: 0.8365\n",
      "Epoch 8/20\n",
      "\u001b[1m968/968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1060 - mse: 0.1060 - val_loss: 0.8318 - val_mse: 0.8318\n",
      "Epoch 9/20\n",
      "\u001b[1m968/968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0940 - mse: 0.0940 - val_loss: 0.8182 - val_mse: 0.8182\n",
      "Epoch 10/20\n",
      "\u001b[1m968/968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0839 - mse: 0.0839 - val_loss: 0.8214 - val_mse: 0.8214\n",
      "Epoch 11/20\n",
      "\u001b[1m968/968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0727 - mse: 0.0727 - val_loss: 0.8237 - val_mse: 0.8237\n",
      "Epoch 12/20\n",
      "\u001b[1m968/968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0688 - mse: 0.0688 - val_loss: 0.8180 - val_mse: 0.8180\n",
      "Epoch 13/20\n",
      "\u001b[1m968/968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0616 - mse: 0.0616 - val_loss: 0.8343 - val_mse: 0.8343\n",
      "Epoch 14/20\n",
      "\u001b[1m968/968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0549 - mse: 0.0549 - val_loss: 0.8276 - val_mse: 0.8276\n",
      "Epoch 15/20\n",
      "\u001b[1m968/968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0464 - mse: 0.0464 - val_loss: 0.8237 - val_mse: 0.8237\n",
      "Epoch 16/20\n",
      "\u001b[1m968/968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0445 - mse: 0.0445 - val_loss: 0.8333 - val_mse: 0.8333\n",
      "Epoch 17/20\n",
      "\u001b[1m968/968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0450 - mse: 0.0450 - val_loss: 0.8235 - val_mse: 0.8235\n",
      "Epoch 18/20\n",
      "\u001b[1m968/968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0432 - mse: 0.0432 - val_loss: 0.8434 - val_mse: 0.8434\n",
      "Epoch 19/20\n",
      "\u001b[1m968/968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0371 - mse: 0.0371 - val_loss: 0.8250 - val_mse: 0.8250\n",
      "Epoch 20/20\n",
      "\u001b[1m968/968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0344 - mse: 0.0344 - val_loss: 0.8292 - val_mse: 0.8292\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Mean Squared Error: 0.811526013120951\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from kerastuner.tuners import RandomSearch\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Sample data\n",
    "sample_df = merged_df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Fill NaN values in text columns with an empty string\n",
    "text_columns = ['text', 'title_x', 'title_y', 'details']\n",
    "sample_df[text_columns] = sample_df[text_columns].fillna('')\n",
    "\n",
    "# Combine text columns into one for TF-IDF vectorization\n",
    "sample_df['combined_text'] = sample_df['text'] + ' ' + sample_df['title_x'] + ' ' + sample_df['title_y'] + ' ' + sample_df['details']\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(sample_df['combined_text'])\n",
    "\n",
    "# Normalize numerical features\n",
    "numerical_features = sample_df[['price', 'average_rating', 'rating_number', 'helpful_vote', 'x_length', 'y_length', 'de_length']]\n",
    "scaler = StandardScaler()\n",
    "normalized_numerical_features = scaler.fit_transform(numerical_features)\n",
    "\n",
    "# Combine TF-IDF features with normalized numerical features\n",
    "X = np.hstack([tfidf_matrix.toarray(), normalized_numerical_features])\n",
    "\n",
    "# Define the target variable\n",
    "y = sample_df['rating'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model-building function\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=hp.Int('units', min_value=32, max_value=512, step=32),\n",
    "                    activation='relu',\n",
    "                    input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dropout(rate=hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    for i in range(hp.Int('num_layers', 1, 3)):\n",
    "        model.add(Dense(units=hp.Int('units_' + str(i), min_value=32, max_value=512, step=32),\n",
    "                        activation='relu'))\n",
    "        model.add(Dropout(rate=hp.Float('dropout_' + str(i), min_value=0.0, max_value=0.5, step=0.1)))\n",
    "\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "# Initialize the Keras Tuner\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='mse',\n",
    "    max_trials=10,  # Increase this for a more thorough search\n",
    "    executions_per_trial=2,\n",
    "    directory='my_dir',\n",
    "    project_name='hyperparam_tuning_ffnn'\n",
    ")\n",
    "\n",
    "# Perform the hyperparameter search\n",
    "tuner.search(X_train, y_train, epochs=10, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal dropout rate is {best_hps.get('dropout')}.\n",
    "\"\"\")\n",
    "\n",
    "# Build the best model\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print the Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Very Similar Peformance between sequential Model and hyper-paramter tuned feed forward Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>Product Recommendation System using bert Embeddings </b><h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T19:36:30.502903Z",
     "iopub.status.busy": "2024-06-09T19:36:30.502012Z",
     "iopub.status.idle": "2024-06-09T20:06:02.548010Z",
     "shell.execute_reply": "2024-06-09T20:06:02.547010Z",
     "shell.execute_reply.started": "2024-06-09T19:36:30.502867Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-09 19:36:43.763201: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-09 19:36:43.763316: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-09 19:36:44.011892: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f947a23aa80941cc9672ab227cb608e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e001242e203c489f83c88b7e2e082281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089c26f9699f4eb28bd2cb1c3d164557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cff5fa54e754cd88d4ed136c1c2c764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19f6133cbb844a98bbd39778812462dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m106/200\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.9849"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717963551.967739     143 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 4.5002 - val_loss: 1.7652\n",
      "Epoch 2/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6832 - val_loss: 1.6377\n",
      "Epoch 3/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5253 - val_loss: 1.5495\n",
      "Epoch 4/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5139 - val_loss: 1.5231\n",
      "Epoch 5/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4428 - val_loss: 1.6609\n",
      "Epoch 6/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3497 - val_loss: 1.9762\n",
      "Epoch 7/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3966 - val_loss: 1.3653\n",
      "Epoch 8/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3365 - val_loss: 1.3940\n",
      "Epoch 9/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3325 - val_loss: 1.3315\n",
      "Epoch 10/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2878 - val_loss: 1.4779\n",
      "Epoch 11/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2937 - val_loss: 1.6386\n",
      "Epoch 12/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1717 - val_loss: 1.4884\n",
      "Epoch 13/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1753 - val_loss: 1.4788\n",
      "Epoch 14/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1432 - val_loss: 1.3524\n",
      "Epoch 15/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1291 - val_loss: 1.4721\n",
      "Epoch 16/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0787 - val_loss: 1.3747\n",
      "Epoch 17/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0047 - val_loss: 1.3140\n",
      "Epoch 18/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1242 - val_loss: 1.3915\n",
      "Epoch 19/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0042 - val_loss: 1.3625\n",
      "Epoch 20/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1237 - val_loss: 1.3921\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Top 5 similar products to 'AlbaChem® PSR II Powdered Dry Cleaning Fluid':\n",
      "Product: Alba Chem PSR Powdered Dry Cleaning Fluid 12.5 oz, Similarity Score: 0.9774683389647245\n",
      "Product: Alba Chem PSR Powdered Dry Cleaning Fluid 12.5 oz, Similarity Score: 0.9766651665965533\n",
      "Product: Bright Air Odor Eliminator - Cool and Clean , 14 Ounce Jar (Pack of 3), Similarity Score: 0.9760934781712813\n",
      "Product: Hydrosilex Recharge 1000 ML/32OZ, Universal Hydrophobic Coating Finish Spray-On Protection for Paint, Vinyl, Rubber, Plastic, Similarity Score: 0.9753441667909953\n",
      "Product: WD40 300080 Specialist Electrical Contact Cleaner 11oz, Similarity Score: 0.9749399656579174\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Select a small subset of the data\n",
    "subset_df = merged_df.sample(n=10000, random_state=42)\n",
    "\n",
    "# Combine text columns into one for BERT embeddings\n",
    "subset_df['combined_text'] = subset_df['text'] + ' ' + subset_df['title_x'] + ' ' + subset_df['title_y'] + ' ' + subset_df['details']\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to get BERT embeddings\n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "    outputs = bert_model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].detach().numpy()\n",
    "    return embeddings\n",
    "\n",
    "# Generate BERT embeddings for the text data\n",
    "bert_embeddings = np.vstack([get_bert_embeddings(text) for text in subset_df['combined_text']])\n",
    "\n",
    "# Normalize numerical features\n",
    "numerical_features = subset_df[['price', 'average_rating', 'rating_number', 'helpful_vote', 'x_length', 'y_length', 'de_length', 'review_length']]\n",
    "scaler = StandardScaler()\n",
    "normalized_numerical_features = scaler.fit_transform(numerical_features)\n",
    "\n",
    "# Combine BERT embeddings with normalized numerical features\n",
    "X = np.hstack([bert_embeddings, normalized_numerical_features])\n",
    "\n",
    "# Define the target variable\n",
    "y = subset_df['rating'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the input data to dense format\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "\n",
    "# Build the regression model\n",
    "input_shape = X_train.shape[1]\n",
    "model = Sequential([\n",
    "    Input(shape=(input_shape,)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_split=0.2, batch_size=32, verbose=1)\n",
    "\n",
    "# Predict ratings for all products\n",
    "predicted_ratings = model.predict(X).flatten()\n",
    "\n",
    "# Combine BERT embeddings, normalized numerical features, and predicted ratings\n",
    "X_with_ratings = np.hstack([bert_embeddings, normalized_numerical_features, predicted_ratings.reshape(-1, 1)])\n",
    "\n",
    "# Calculate cosine similarity between products\n",
    "cosine_sim_with_ratings = cosine_similarity(X_with_ratings, X_with_ratings)\n",
    "\n",
    "# Function to get top K similar products considering predicted ratings\n",
    "def get_similar_products(product_idx, k=5):\n",
    "    sim_scores = list(enumerate(cosine_sim_with_ratings[product_idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:k+1]  # Exclude the product itself\n",
    "    similar_products = [(subset_df.iloc[i[0]]['title_y'], i[1]) for i in sim_scores]\n",
    "    return similar_products\n",
    "\n",
    "# Example usage\n",
    "example_product_idx = 0  # Using the first product as an example\n",
    "similar_products = get_similar_products(example_product_idx, k=5)\n",
    "print(f\"Top 5 similar products to '{subset_df.iloc[example_product_idx]['title_y']}':\")\n",
    "for product, score in similar_products:\n",
    "    print(f\"Product: {product}, Similarity Score: {score}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5142826,
     "sourceId": 8596538,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5170078,
     "sourceId": 8634029,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30716,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
